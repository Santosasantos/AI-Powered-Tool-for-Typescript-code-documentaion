{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Hyper parameter tuning search\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "iKGqbHFv8RrE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnDS3iiwm9mr"
      },
      "outputs": [],
      "source": [
        "def hyperparameter_search(train_data, val_data, base_output_dir, n_trials=4):\n",
        "    \"\"\"Perform hyperparameter search to find optimal training configuration\"\"\"\n",
        "\n",
        "    import optuna\n",
        "    from datetime import datetime\n",
        "\n",
        "    # Create directory for search results\n",
        "    search_dir = f\"{base_output_dir}/hparam_search_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "    os.makedirs(search_dir, exist_ok=True)\n",
        "\n",
        "    # Define the objective function\n",
        "    def objective(trial):\n",
        "        # Sample hyperparameters to explore\n",
        "        lr = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
        "        bs = trial.suggest_categorical(\"batch_size\", [2, 4, 8])\n",
        "        grad_accum = trial.suggest_categorical(\"gradient_accumulation_steps\", [2, 4, 8])\n",
        "\n",
        "        # Create trial directory\n",
        "        trial_dir = f\"{search_dir}/trial_{trial.number}\"\n",
        "        os.makedirs(trial_dir, exist_ok=True)\n",
        "\n",
        "        # Use subset of data for faster exploration\n",
        "        train_subset = train_data[:min(300, len(train_data))]\n",
        "        val_subset = val_data[:min(100, len(val_data))]\n",
        "\n",
        "        # Log hyperparameters\n",
        "        with open(f\"{trial_dir}/params.json\", 'w') as f:\n",
        "            json.dump({\n",
        "                \"learning_rate\": lr,\n",
        "                \"batch_size\": bs,\n",
        "                \"gradient_accumulation_steps\": grad_accum,\n",
        "            }, f, indent=2)\n",
        "\n",
        "        # Train with this configuration\n",
        "        try:\n",
        "            print(f\"\\nTrial {trial.number}: lr={lr}, bs={bs}, grad_accum={grad_accum}\")\n",
        "            model, tokenizer, trainer = train_on_a100(\n",
        "                train_data=train_subset,\n",
        "                val_data=val_subset,\n",
        "                output_dir=trial_dir,\n",
        "                model_name=\"Salesforce/codet5-base\",\n",
        "                epochs=3,  # Use fewer epochs for search\n",
        "                batch_size=bs,\n",
        "                gradient_accumulation_steps=grad_accum,\n",
        "                learning_rate=lr,\n",
        "                fp16=True\n",
        "            )\n",
        "\n",
        "            # Get the final validation score\n",
        "            metrics = [x for x in trainer.state.log_history if 'eval_rougeL' in x]\n",
        "            if not metrics:\n",
        "                return 0.0\n",
        "\n",
        "            best_rouge_l = max(x['eval_rougeL'] for x in metrics)\n",
        "\n",
        "            # Save result\n",
        "            with open(f\"{trial_dir}/result.json\", 'w') as f:\n",
        "                json.dump({\n",
        "                    \"best_eval_rougeL\": best_rouge_l,\n",
        "                    \"final_metrics\": metrics[-1] if metrics else None\n",
        "                }, f, indent=2)\n",
        "\n",
        "            return best_rouge_l\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in trial {trial.number}: {e}\")\n",
        "            return 0.0\n",
        "\n",
        "    # Create study and optimize\n",
        "    study = optuna.create_study(direction=\"maximize\")\n",
        "    study.optimize(objective, n_trials=n_trials)\n",
        "\n",
        "    # Save best parameters\n",
        "    with open(f\"{search_dir}/best_params.json\", 'w') as f:\n",
        "        json.dump({\n",
        "            \"best_params\": study.best_params,\n",
        "            \"best_value\": study.best_value,\n",
        "            \"best_trial\": study.best_trial.number\n",
        "        }, f, indent=2)\n",
        "\n",
        "    print(f\"\\nBest hyperparameters: {study.best_params}\")\n",
        "    print(f\"Best RougeL: {study.best_value:.4f}\")\n",
        "\n",
        "    # Train final model with best parameters\n",
        "    best_model_dir = f\"{search_dir}/best_model\"\n",
        "    os.makedirs(best_model_dir, exist_ok=True)\n",
        "\n",
        "    final_model, final_tokenizer, _ = train_on_a100(\n",
        "        train_data=train_data,\n",
        "        val_data=val_data,\n",
        "        output_dir=best_model_dir,\n",
        "        model_name=\"Salesforce/codet5-base\",\n",
        "        epochs=8,\n",
        "        batch_size=study.best_params['batch_size'],\n",
        "        gradient_accumulation_steps=study.best_params['gradient_accumulation_steps'],\n",
        "        learning_rate=study.best_params['learning_rate'],\n",
        "        fp16=True\n",
        "    )\n",
        "\n",
        "    return best_model_dir, study.best_params"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_test_set(model_path, test_data, output_dir=None):\n",
        "    \"\"\"Perform comprehensive evaluation on the full test set with metrics and examples\"\"\"\n",
        "\n",
        "    if output_dir is None:\n",
        "        output_dir = os.path.dirname(model_path) + \"/test_evaluation\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Track metrics\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    all_scores = {\n",
        "        'rouge1': [], 'rouge2': [], 'rougeL': [],\n",
        "        'code_length': [], 'reference_length': [], 'generated_length': []\n",
        "    }\n",
        "\n",
        "    # Category classifications\n",
        "    categories = {\n",
        "        'excellent': 0,  # RougeL > 0.9\n",
        "        'good': 0,       # RougeL > 0.7\n",
        "        'moderate': 0,   # RougeL > 0.5\n",
        "        'poor': 0        # RougeL <= 0.5\n",
        "    }\n",
        "\n",
        "    # Detailed results for each example\n",
        "    detailed_results = []\n",
        "\n",
        "    print(f\"Evaluating model on {len(test_data)} test examples...\")\n",
        "    batch_size = 8\n",
        "\n",
        "    for i in range(0, len(test_data), batch_size):\n",
        "        batch_end = min(i + batch_size, len(test_data))\n",
        "        batch = test_data[i:batch_end]\n",
        "\n",
        "        for example in batch:\n",
        "            # Get code and reference\n",
        "            code = example['input']\n",
        "            reference = example['output']\n",
        "\n",
        "            # Generate documentation\n",
        "            input_text = f\"Generate documentation for TypeScript code: {code}\"\n",
        "            inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    inputs.input_ids,\n",
        "                    max_length=256,\n",
        "                    num_beams=4,\n",
        "                    early_stopping=True\n",
        "                )\n",
        "\n",
        "            generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            # Calculate metrics\n",
        "            scores = scorer.score(reference, generated)\n",
        "\n",
        "            # Track lengths\n",
        "            all_scores['code_length'].append(len(code.split()))\n",
        "            all_scores['reference_length'].append(len(reference.split()))\n",
        "            all_scores['generated_length'].append(len(generated.split()))\n",
        "\n",
        "            # Track ROUGE scores\n",
        "            for key in ['rouge1', 'rouge2', 'rougeL']:\n",
        "                all_scores[key].append(scores[key].fmeasure)\n",
        "\n",
        "            # Categorize result\n",
        "            rouge_l = scores['rougeL'].fmeasure\n",
        "            if rouge_l > 0.9:\n",
        "                categories['excellent'] += 1\n",
        "            elif rouge_l > 0.7:\n",
        "                categories['good'] += 1\n",
        "            elif rouge_l > 0.5:\n",
        "                categories['moderate'] += 1\n",
        "            else:\n",
        "                categories['poor'] += 1\n",
        "\n",
        "            # Save detailed result\n",
        "            detailed_results.append({\n",
        "                'code': code,\n",
        "                'reference': reference,\n",
        "                'generated': generated,\n",
        "                'rouge1': scores['rouge1'].fmeasure,\n",
        "                'rouge2': scores['rouge2'].fmeasure,\n",
        "                'rougeL': scores['rougeL'].fmeasure,\n",
        "                'category': next(k for k, v in {'excellent': 0.9, 'good': 0.7, 'moderate': 0.5, 'poor': 0}\n",
        "                                 .items() if rouge_l > v)\n",
        "            })\n",
        "\n",
        "        # Log progress\n",
        "        if (i // batch_size) % 5 == 0:\n",
        "            print(f\"Processed {batch_end}/{len(test_data)} examples\")\n",
        "\n",
        "    # Calculate average metrics\n",
        "    avg_metrics = {\n",
        "        key: sum(values) / len(values) if values else 0\n",
        "        for key, values in all_scores.items()\n",
        "    }\n",
        "\n",
        "    # Category percentages\n",
        "    total = len(test_data)\n",
        "    category_pcts = {k: 100 * v / total for k, v in categories.items()}\n",
        "\n",
        "    # Save results\n",
        "    results = {\n",
        "        'avg_metrics': avg_metrics,\n",
        "        'categories': categories,\n",
        "        'category_percentages': category_pcts,\n",
        "        'detailed_results': detailed_results[:20]  # Save only first 20 for space\n",
        "    }\n",
        "\n",
        "    with open(f\"{output_dir}/test_results.json\", 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "\n",
        "    # Generate HTML report\n",
        "    html_report = f\"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head>\n",
        "        <title>TypeScript Documentation Model Evaluation</title>\n",
        "        <style>\n",
        "            body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
        "            .header {{ background-color: #f0f0f0; padding: 15px; border-radius: 5px; }}\n",
        "            .metrics {{ margin: 20px 0; }}\n",
        "            .categories {{ display: flex; justify-content: space-between; margin: 20px 0; }}\n",
        "            .category {{ text-align: center; padding: 10px; border-radius: 5px; }}\n",
        "            .excellent {{ background-color: #d4edda; }}\n",
        "            .good {{ background-color: #d1ecf1; }}\n",
        "            .moderate {{ background-color: #fff3cd; }}\n",
        "            .poor {{ background-color: #f8d7da; }}\n",
        "            table {{ width: 100%; border-collapse: collapse; }}\n",
        "            th, td {{ border: 1px solid #ddd; padding: 8px; }}\n",
        "            tr:nth-child(even) {{ background-color: #f2f2f2; }}\n",
        "            th {{ background-color: #4CAF50; color: white; }}\n",
        "            .example {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}\n",
        "            pre {{ background-color: #f5f5f5; padding: 10px; border-radius: 5px; overflow-x: auto; }}\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <div class=\"header\">\n",
        "            <h1>TypeScript Documentation Model Evaluation</h1>\n",
        "            <p>Evaluation Date: {time.strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"metrics\">\n",
        "            <h2>Overall Metrics</h2>\n",
        "            <table>\n",
        "                <tr>\n",
        "                    <th>Metric</th>\n",
        "                    <th>Value</th>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>Average ROUGE-1</td>\n",
        "                    <td>{avg_metrics['rouge1']:.4f}</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>Average ROUGE-2</td>\n",
        "                    <td>{avg_metrics['rouge2']:.4f}</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>Average ROUGE-L</td>\n",
        "                    <td>{avg_metrics['rougeL']:.4f}</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>Average Code Length (words)</td>\n",
        "                    <td>{avg_metrics['code_length']:.1f}</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>Average Reference Doc Length (words)</td>\n",
        "                    <td>{avg_metrics['reference_length']:.1f}</td>\n",
        "                </tr>\n",
        "                <tr>\n",
        "                    <td>Average Generated Doc Length (words)</td>\n",
        "                    <td>{avg_metrics['generated_length']:.1f}</td>\n",
        "                </tr>\n",
        "            </table>\n",
        "        </div>\n",
        "\n",
        "        <div class=\"categories\">\n",
        "            <div class=\"category excellent\">\n",
        "                <h3>Excellent</h3>\n",
        "                <p>{categories['excellent']} examples</p>\n",
        "                <p>({category_pcts['excellent']:.1f}%)</p>\n",
        "            </div>\n",
        "            <div class=\"category good\">\n",
        "                <h3>Good</h3>\n",
        "                <p>{categories['good']} examples</p>\n",
        "                <p>({category_pcts['good']:.1f}%)</p>\n",
        "            </div>\n",
        "            <div class=\"category moderate\">\n",
        "                <h3>Moderate</h3>\n",
        "                <p>{categories['moderate']} examples</p>\n",
        "                <p>({category_pcts['moderate']:.1f}%)</p>\n",
        "            </div>\n",
        "            <div class=\"category poor\">\n",
        "                <h3>Poor</h3>\n",
        "                <p>{categories['poor']} examples</p>\n",
        "                <p>({category_pcts['poor']:.1f}%)</p>\n",
        "            </div>\n",
        "        </div>\n",
        "\n",
        "        <h2>Example Results</h2>\n",
        "    \"\"\"\n",
        "\n",
        "    # Add a few examples from each category\n",
        "    for category in ['excellent', 'good', 'moderate', 'poor']:\n",
        "        examples = [r for r in detailed_results if r['category'] == category][:2]\n",
        "\n",
        "        html_report += f\"\"\"\n",
        "        <h3>{category.title()} Examples</h3>\n",
        "        \"\"\"\n",
        "\n",
        "        for i, example in enumerate(examples):\n",
        "            html_report += f\"\"\"\n",
        "            <div class=\"example {category}\">\n",
        "                <h4>Example {i+1}</h4>\n",
        "                <h5>Code:</h5>\n",
        "                <pre>{example['code']}</pre>\n",
        "\n",
        "                <h5>Generated Documentation:</h5>\n",
        "                <pre>{example['generated']}</pre>\n",
        "\n",
        "                <h5>Reference Documentation:</h5>\n",
        "                <pre>{example['reference']}</pre>\n",
        "\n",
        "                <p>\n",
        "                    <strong>ROUGE-1:</strong> {example['rouge1']:.4f}\n",
        "                    <strong>ROUGE-2:</strong> {example['rouge2']:.4f}\n",
        "                    <strong>ROUGE-L:</strong> {example['rougeL']:.4f}\n",
        "                </p>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "\n",
        "    html_report += \"\"\"\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "\n",
        "    # Save HTML report\n",
        "    with open(f\"{output_dir}/evaluation_report.html\", 'w') as f:\n",
        "        f.write(html_report)\n",
        "\n",
        "    print(f\"\\nTest evaluation completed. Results saved to {output_dir}\")\n",
        "    print(f\"Summary metrics:\")\n",
        "    print(f\"- ROUGE-L: {avg_metrics['rougeL']:.4f}\")\n",
        "    print(f\"- Quality breakdown: {categories['excellent']} excellent, {categories['good']} good, \" +\n",
        "          f\"{categories['moderate']} moderate, {categories['poor']} poor\")\n",
        "\n",
        "    return avg_metrics, categories"
      ],
      "metadata": {
        "id": "NPuOd1ep78fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def implement_enhanced_typescript_documentation():\n",
        "    \"\"\"Implement all enhancements for the TypeScript documentation generator\"\"\"\n",
        "\n",
        "    # Set up directories\n",
        "    base_dir = '/content/drive/MyDrive/ts_documentation'\n",
        "    enhanced_model_dir = f'{base_dir}/models/enhanced_codet5'\n",
        "    os.makedirs(enhanced_model_dir, exist_ok=True)\n",
        "\n",
        "    # Load data\n",
        "    with open(f'{base_dir}/data/full_train_split.json', 'r') as f:\n",
        "        train_data = json.load(f)\n",
        "\n",
        "    with open(f'{base_dir}/data/full_val_split.json', 'r') as f:\n",
        "        val_data = json.load(f)\n",
        "\n",
        "    with open(f'{base_dir}/data/full_test_split.json', 'r') as f:\n",
        "        test_data = json.load(f)\n",
        "\n",
        "    print(f\"Loaded {len(train_data)} training, {len(val_data)} validation, and {len(test_data)} test examples\")\n",
        "\n",
        "    # STEP 1: Find optimal hyperparameters (smaller search for demonstration)\n",
        "    print(\"\\n=== ENHANCEMENT 1: Hyperparameter Optimization ===\")\n",
        "    # Install optuna if needed\n",
        "    try:\n",
        "        import optuna\n",
        "    except ImportError:\n",
        "        !pip install optuna\n",
        "        import optuna\n",
        "\n",
        "    # Run hyperparameter search (use small n_trials for demonstration)\n",
        "    best_model_dir, best_params = hyperparameter_search(\n",
        "        train_data=train_data[:500],  # Use subset for faster search\n",
        "        val_data=val_data,\n",
        "        base_output_dir=enhanced_model_dir,\n",
        "        n_trials=4  # Increase for better results\n",
        "    )\n",
        "\n",
        "    # STEP 2: Comprehensive test evaluation\n",
        "    print(\"\\n=== ENHANCEMENT 2: Comprehensive Test Evaluation ===\")\n",
        "    avg_metrics, categories = evaluate_test_set(\n",
        "        model_path=best_model_dir,\n",
        "        test_data=test_data,\n",
        "        output_dir=f\"{enhanced_model_dir}/test_evaluation\"\n",
        "    )\n",
        "\n",
        "    # STEP 3: Generate enhanced documentation examples\n",
        "    print(\"\\n=== ENHANCEMENT 3: Enhanced Documentation Generation ===\")\n",
        "    example_code = \"\"\"\n",
        "    export function formatDate(date: Date, format: string = 'YYYY-MM-DD'): string {\n",
        "      const year = date.getFullYear();\n",
        "      const month = String(date.getMonth() + 1).padStart(2, '0');\n",
        "      const day = String(date.getDate()).padStart(2, '0');\n",
        "\n",
        "      let result = format;\n",
        "      result = result.replace('YYYY', String(year));\n",
        "      result = result.replace('MM', month);\n",
        "      result = result.replace('DD', day);\n",
        "\n",
        "      return result;\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\nOriginal code:\")\n",
        "    print(example_code)\n",
        "\n",
        "    # Generate different documentation styles\n",
        "    for style in [\"standard\", \"jsdoc\", \"detailed\", \"markdown\"]:\n",
        "        print(f\"\\n--- {style.upper()} Documentation Style ---\")\n",
        "        enhanced_doc = generate_comprehensive_docs(\n",
        "            model_path=best_model_dir,\n",
        "            code_example=example_code,\n",
        "            doc_style=style\n",
        "        )\n",
        "        print(enhanced_doc)\n",
        "\n",
        "    print(\"\\nEnhancements implementation completed!\")\n",
        "    return {\n",
        "        \"best_model_dir\": best_model_dir,\n",
        "        \"best_params\": best_params,\n",
        "        \"test_metrics\": avg_metrics\n",
        "    }\n",
        "\n",
        "# Run the implementation\n",
        "# implementation_results = implement_enhanced_typescript_documentation()"
      ],
      "metadata": {
        "id": "JQtkijVx8BeU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna  # For hyperparameter search\n",
        "implementation_results = implement_enhanced_typescript_documentation()"
      ],
      "metadata": {
        "id": "clc2SQJp8GK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_enhanced_templates():\n",
        "    \"\"\"Create improved documentation templates for different styles\"\"\"\n",
        "\n",
        "    # Dictionary of template formats with improved prompts\n",
        "    templates = {\n",
        "        \"standard\": {\n",
        "            \"prompt\": \"Generate clear, accurate TypeScript documentation for this code. \" +\n",
        "                     \"Focus only on the function or class name, parameters, return type, and purpose. \" +\n",
        "                     \"Do not include any import paths or external references. CODE: {code}\",\n",
        "            \"postprocess\": standard_postprocessor\n",
        "        },\n",
        "        \"jsdoc\": {\n",
        "            \"prompt\": \"Generate JSDoc documentation for this TypeScript code. \" +\n",
        "                     \"Include properly formatted @param tags for each parameter with accurate type information, \" +\n",
        "                     \"an @returns tag with the correct return type, and a clear description of functionality. \" +\n",
        "                     \"Exclude any import paths. Always extract the exact function name. CODE: {code}\",\n",
        "            \"postprocess\": jsdoc_postprocessor\n",
        "        },\n",
        "        \"markdown\": {\n",
        "            \"prompt\": \"Generate markdown documentation with precise headings for this TypeScript code. \" +\n",
        "                     \"Include function name as title, followed by Parameters section with bullet points \" +\n",
        "                     \"for each parameter showing name and type, Returns section with correct type, \" +\n",
        "                     \"and Example section with sample usage code. Ensure exact function name is used. CODE: {code}\",\n",
        "            \"postprocess\": markdown_postprocessor\n",
        "        }\n",
        "    }\n",
        "\n",
        "    return templates\n",
        "\n",
        "def standard_postprocessor(generated_doc, original_code):\n",
        "    \"\"\"Post-process standard documentation to fix common issues\"\"\"\n",
        "    # Extract actual function/class name from original code\n",
        "    import re\n",
        "\n",
        "    # Find function or class name\n",
        "    fn_match = re.search(r'function\\s+(\\w+)', original_code)\n",
        "    class_match = re.search(r'class\\s+(\\w+)', original_code)\n",
        "    interface_match = re.search(r'interface\\s+(\\w+)', original_code)\n",
        "\n",
        "    entity_name = None\n",
        "    if fn_match:\n",
        "        entity_name = fn_match.group(1)\n",
        "    elif class_match:\n",
        "        entity_name = class_match.group(1)\n",
        "    elif interface_match:\n",
        "        entity_name = interface_match.group(1)\n",
        "\n",
        "    # If we found a name, ensure it's correct in the documentation\n",
        "    if entity_name:\n",
        "        # Replace any incorrect function names (e.g., formatFormat -> formatDate)\n",
        "        generated_doc = re.sub(r'\\*\\*\\w+\\*\\*', f'**{entity_name}**', generated_doc)\n",
        "        generated_doc = re.sub(r'`\\w+\\(', f'`{entity_name}(', generated_doc)\n",
        "\n",
        "    # Remove any import paths\n",
        "    generated_doc = re.sub(r'import\\(\".*?\"\\)\\.', '', generated_doc)\n",
        "\n",
        "    return generated_doc\n",
        "\n",
        "def jsdoc_postprocessor(generated_doc, original_code):\n",
        "    \"\"\"Post-process JSDoc documentation to fix common issues\"\"\"\n",
        "    # Similar to standard postprocessor, with JSDoc-specific fixes\n",
        "    import re\n",
        "\n",
        "    # Find function or class name\n",
        "    fn_match = re.search(r'function\\s+(\\w+)', original_code)\n",
        "    class_match = re.search(r'class\\s+(\\w+)', original_code)\n",
        "\n",
        "    entity_name = None\n",
        "    if fn_match:\n",
        "        entity_name = fn_match.group(1)\n",
        "    elif class_match:\n",
        "        entity_name = class_match.group(1)\n",
        "\n",
        "    # If we found a name, ensure it's correct in the documentation\n",
        "    if entity_name:\n",
        "        # Replace function name in description\n",
        "        generated_doc = re.sub(r'\\*\\*\\w+\\*\\*', f'**{entity_name}**', generated_doc)\n",
        "        generated_doc = re.sub(r'`\\w+\\(', f'`{entity_name}(', generated_doc)\n",
        "\n",
        "    # Extract parameters from original code to ensure accurate @param tags\n",
        "    params_match = re.search(r'\\(([^)]*)\\)', original_code)\n",
        "    if params_match and entity_name:\n",
        "        params_text = params_match.group(1).strip()\n",
        "        param_list = []\n",
        "\n",
        "        if params_text:\n",
        "            for param in params_text.split(','):\n",
        "                param = param.strip()\n",
        "                if param:\n",
        "                    # Extract parameter name and type\n",
        "                    param_parts = param.split(':')\n",
        "                    param_name = param_parts[0].strip()\n",
        "                    param_type = param_parts[1].strip() if len(param_parts) > 1 else \"any\"\n",
        "\n",
        "                    # Clean up default values\n",
        "                    if '=' in param_name:\n",
        "                        param_name = param_name.split('=')[0].strip()\n",
        "\n",
        "                    param_list.append((param_name, param_type))\n",
        "\n",
        "        # Check if @param tags are present or correct, fix if needed\n",
        "        if not '@param' in generated_doc:\n",
        "            # Add params if missing\n",
        "            param_block = \"\"\n",
        "            for name, type_info in param_list:\n",
        "                param_block += f\" * @param {name} - Parameter of type {type_info}\\n\"\n",
        "\n",
        "            # Add to JSDoc before closing */\n",
        "            if '*/' in generated_doc:\n",
        "                generated_doc = generated_doc.replace('*/', param_block + ' */')\n",
        "\n",
        "    # Remove any import paths\n",
        "    generated_doc = re.sub(r'import\\(\".*?\"\\)\\.', '', generated_doc)\n",
        "\n",
        "    return generated_doc\n",
        "\n",
        "def markdown_postprocessor(generated_doc, original_code):\n",
        "    \"\"\"Post-process markdown documentation to fix common issues\"\"\"\n",
        "    # Similar process but for markdown formatting\n",
        "    import re\n",
        "\n",
        "    # Find function or class name\n",
        "    fn_match = re.search(r'function\\s+(\\w+)', original_code)\n",
        "    class_match = re.search(r'class\\s+(\\w+)', original_code)\n",
        "\n",
        "    entity_name = None\n",
        "    if fn_match:\n",
        "        entity_name = fn_match.group(1)\n",
        "    elif class_match:\n",
        "        entity_name = class_match.group(1)\n",
        "\n",
        "    # If we found a name, ensure it's correct in the documentation\n",
        "    if entity_name:\n",
        "        # Fix the title\n",
        "        if re.search(r'^#\\s+\\w+', generated_doc):\n",
        "            generated_doc = re.sub(r'^#\\s+\\w+', f'# {entity_name}', generated_doc)\n",
        "        else:\n",
        "            generated_doc = f\"# {entity_name}\\n\\n\" + generated_doc\n",
        "\n",
        "    # Remove any import paths\n",
        "    generated_doc = re.sub(r'import\\(\".*?\"\\)\\.', '', generated_doc)\n",
        "\n",
        "    # Ensure parameters section exists\n",
        "    if \"## Parameters\" not in generated_doc:\n",
        "        # Extract parameters from original code\n",
        "        params_match = re.search(r'\\(([^)]*)\\)', original_code)\n",
        "        if params_match:\n",
        "            params_text = params_match.group(1).strip()\n",
        "            param_section = \"\\n## Parameters\\n\\n\"\n",
        "\n",
        "            if params_text:\n",
        "                for param in params_text.split(','):\n",
        "                    param = param.strip()\n",
        "                    if param:\n",
        "                        # Extract parameter name and type\n",
        "                        param_parts = param.split(':')\n",
        "                        param_name = param_parts[0].strip()\n",
        "                        param_type = param_parts[1].strip() if len(param_parts) > 1 else \"any\"\n",
        "\n",
        "                        # Clean up default values\n",
        "                        if '=' in param_name:\n",
        "                            param_name, default = param_name.split('=')\n",
        "                            param_name = param_name.strip()\n",
        "                            param_type += f\" = {default.strip()}\"\n",
        "\n",
        "                        param_section += f\"- **{param_name}** (`{param_type}`): Description of parameter\\n\"\n",
        "            else:\n",
        "                param_section += \"- No parameters\\n\"\n",
        "\n",
        "            # Add parameters section after description\n",
        "            generated_doc += param_section\n",
        "\n",
        "    # Ensure returns section exists for functions\n",
        "    if \"## Returns\" not in generated_doc and fn_match:\n",
        "        # Extract return type from original code\n",
        "        return_match = re.search(r'\\):\\s*([^{]+)', original_code)\n",
        "        returns_section = \"\\n## Returns\\n\\n\"\n",
        "\n",
        "        if return_match:\n",
        "            return_type = return_match.group(1).strip()\n",
        "            returns_section += f\"- `{return_type}`: Return value description\\n\"\n",
        "        else:\n",
        "            returns_section += \"- `void`: This function doesn't return a value\\n\"\n",
        "\n",
        "        # Add returns section\n",
        "        generated_doc += returns_section\n",
        "\n",
        "    # Ensure example section exists\n",
        "    if \"## Example\" not in generated_doc and entity_name:\n",
        "        example_section = \"\\n## Example\\n\\n```typescript\\n// Example usage of \" + entity_name + \"\\n```\\n\"\n",
        "        generated_doc += example_section\n",
        "\n",
        "    return generated_doc"
      ],
      "metadata": {
        "id": "JL80Iikx80Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import RobertaTokenizer, T5ForConditionalGeneration\n",
        "from rouge_score import rouge_scorer\n",
        "from collections import defaultdict\n",
        "\n",
        "def run_comprehensive_evaluation(model_path, test_data_path, output_dir):\n",
        "    \"\"\"\n",
        "    Perform a detailed evaluation of the TypeScript documentation model.\n",
        "\n",
        "    Args:\n",
        "        model_path: Path to the fine-tuned model\n",
        "        test_data_path: Path to the test data JSON\n",
        "        output_dir: Directory to save evaluation results\n",
        "    \"\"\"\n",
        "    print(f\"Starting comprehensive evaluation of model at {model_path}\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Load test data\n",
        "    with open(test_data_path, 'r') as f:\n",
        "        test_data = json.load(f)\n",
        "\n",
        "    print(f\"Loaded {len(test_data)} test examples\")\n",
        "\n",
        "    # Load model and tokenizer\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(model_path)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize scorer and metrics\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    # Prepare metrics collection\n",
        "    all_results = []\n",
        "    metrics_by_length = defaultdict(list)\n",
        "    metrics_by_complexity = defaultdict(list)\n",
        "\n",
        "    # Define documentation styles for comparison\n",
        "    doc_styles = {\n",
        "        \"standard\": \"Generate documentation for TypeScript code: \",\n",
        "        \"jsdoc\": \"Generate JSDoc-style documentation with @param, @returns, and @example tags for TypeScript code: \",\n",
        "        \"detailed\": \"Generate detailed documentation explaining purpose, parameters, return types, and usage examples for TypeScript code: \",\n",
        "        \"markdown\": \"Generate markdown documentation with sections for Parameters, Returns, and Examples for TypeScript code: \"\n",
        "    }\n",
        "\n",
        "    # Run evaluation\n",
        "    print(\"Starting evaluation...\")\n",
        "    for idx, example in enumerate(tqdm(test_data[:min(len(test_data), 200)])):  # Limit to 200 examples to keep it manageable\n",
        "        code = example['input']\n",
        "        reference = example['output']\n",
        "\n",
        "        # Calculate code complexity (using length as a simple proxy)\n",
        "        code_length = len(code.split())\n",
        "        if code_length < 50:\n",
        "            complexity = \"simple\"\n",
        "        elif code_length < 150:\n",
        "            complexity = \"medium\"\n",
        "        else:\n",
        "            complexity = \"complex\"\n",
        "\n",
        "        # Evaluate across different documentation styles\n",
        "        style_results = {}\n",
        "        for style_name, style_prompt in doc_styles.items():\n",
        "            input_text = f\"{style_prompt}{code}\"\n",
        "            inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "\n",
        "            # Generate documentation\n",
        "            with torch.no_grad():\n",
        "                output_sequences = model.generate(\n",
        "                    inputs.input_ids,\n",
        "                    max_length=256,\n",
        "                    num_beams=4,\n",
        "                    do_sample=True,\n",
        "                    temperature=0.7 if style_name != \"standard\" else 0.5,  # Allow more creativity for detailed styles\n",
        "                    early_stopping=True\n",
        "                )\n",
        "\n",
        "            # Decode generated text\n",
        "            generated = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
        "\n",
        "            # Calculate metrics\n",
        "            scores = scorer.score(reference, generated)\n",
        "\n",
        "            style_results[style_name] = {\n",
        "                \"generated\": generated,\n",
        "                \"rouge1\": scores['rouge1'].fmeasure,\n",
        "                \"rouge2\": scores['rouge2'].fmeasure,\n",
        "                \"rougeL\": scores['rougeL'].fmeasure,\n",
        "                \"length\": len(generated.split())\n",
        "            }\n",
        "\n",
        "            # Track metrics by code length and complexity\n",
        "            metrics_by_length[code_length].append(scores['rougeL'].fmeasure)\n",
        "            metrics_by_complexity[complexity].append(scores['rougeL'].fmeasure)\n",
        "\n",
        "        # Store result\n",
        "        result = {\n",
        "            \"id\": idx,\n",
        "            \"code\": code,\n",
        "            \"reference\": reference,\n",
        "            \"code_length\": code_length,\n",
        "            \"complexity\": complexity,\n",
        "            \"styles\": style_results\n",
        "        }\n",
        "        all_results.append(result)\n",
        "\n",
        "        # Save results periodically\n",
        "        if (idx + 1) % 20 == 0:\n",
        "            with open(f\"{output_dir}/results_partial.json\", 'w') as f:\n",
        "                json.dump(all_results, f, indent=2)\n",
        "\n",
        "    # Save full results\n",
        "    with open(f\"{output_dir}/evaluation_results.json\", 'w') as f:\n",
        "        json.dump(all_results, f, indent=2)\n",
        "\n",
        "    # Calculate overall metrics\n",
        "    style_metrics = {style: [] for style in doc_styles.keys()}\n",
        "    for result in all_results:\n",
        "        for style, metrics in result[\"styles\"].items():\n",
        "            style_metrics[style].append({\n",
        "                \"rouge1\": metrics[\"rouge1\"],\n",
        "                \"rouge2\": metrics[\"rouge2\"],\n",
        "                \"rougeL\": metrics[\"rougeL\"],\n",
        "                \"length\": metrics[\"length\"]\n",
        "            })\n",
        "\n",
        "    # Average metrics by style\n",
        "    avg_metrics = {}\n",
        "    for style, metrics_list in style_metrics.items():\n",
        "        avg_metrics[style] = {\n",
        "            \"rouge1\": np.mean([m[\"rouge1\"] for m in metrics_list]),\n",
        "            \"rouge2\": np.mean([m[\"rouge2\"] for m in metrics_list]),\n",
        "            \"rougeL\": np.mean([m[\"rougeL\"] for m in metrics_list]),\n",
        "            \"length\": np.mean([m[\"length\"] for m in metrics_list])\n",
        "        }\n",
        "\n",
        "    # Save average metrics\n",
        "    with open(f\"{output_dir}/avg_metrics.json\", 'w') as f:\n",
        "        json.dump(avg_metrics, f, indent=2)\n",
        "\n",
        "    # Generate visualizations\n",
        "    generate_visualizations(all_results, avg_metrics, metrics_by_complexity, output_dir)\n",
        "\n",
        "    # Create HTML report\n",
        "    create_html_report(all_results, avg_metrics, metrics_by_complexity, output_dir)\n",
        "\n",
        "    print(f\"Evaluation complete! Results saved to {output_dir}\")\n",
        "    return avg_metrics\n",
        "\n",
        "def generate_visualizations(results, avg_metrics, metrics_by_complexity, output_dir):\n",
        "    \"\"\"Generate visualization charts for the evaluation results\"\"\"\n",
        "    os.makedirs(f\"{output_dir}/charts\", exist_ok=True)\n",
        "\n",
        "    # Style comparison bar chart\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    styles = list(avg_metrics.keys())\n",
        "    rouge_l_scores = [avg_metrics[style][\"rougeL\"] for style in styles]\n",
        "\n",
        "    bars = plt.bar(styles, rouge_l_scores, color='skyblue')\n",
        "\n",
        "    # Add data labels\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                 f'{height:.4f}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "    plt.title('ROUGE-L Scores by Documentation Style')\n",
        "    plt.xlabel('Documentation Style')\n",
        "    plt.ylabel('Average ROUGE-L Score')\n",
        "    plt.ylim(0, 1.0)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.savefig(f\"{output_dir}/charts/style_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "\n",
        "    # Complexity comparison\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    complexity_categories = [\"simple\", \"medium\", \"complex\"]\n",
        "    complexity_scores = [np.mean(metrics_by_complexity[cat]) for cat in complexity_categories]\n",
        "\n",
        "    bars = plt.bar(complexity_categories, complexity_scores, color='lightgreen')\n",
        "\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                 f'{height:.4f}', ha='center', va='bottom', fontsize=10)\n",
        "\n",
        "    plt.title('ROUGE-L Scores by Code Complexity')\n",
        "    plt.xlabel('Code Complexity')\n",
        "    plt.ylabel('Average ROUGE-L Score')\n",
        "    plt.ylim(0, 1.0)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.savefig(f\"{output_dir}/charts/complexity_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "\n",
        "    # Create a style vs. complexity heatmap\n",
        "    complexity_style_matrix = {}\n",
        "    for complexity in [\"simple\", \"medium\", \"complex\"]:\n",
        "        complexity_style_matrix[complexity] = {}\n",
        "        for style in avg_metrics.keys():\n",
        "            # Filter results by complexity and style\n",
        "            filtered_scores = []\n",
        "            for result in results:\n",
        "                if result[\"complexity\"] == complexity:\n",
        "                    filtered_scores.append(result[\"styles\"][style][\"rougeL\"])\n",
        "\n",
        "            if filtered_scores:\n",
        "                complexity_style_matrix[complexity][style] = np.mean(filtered_scores)\n",
        "            else:\n",
        "                complexity_style_matrix[complexity][style] = 0\n",
        "\n",
        "    # Convert to DataFrame for heatmap\n",
        "    df = pd.DataFrame(complexity_style_matrix).T\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(df, annot=True, cmap=\"YlGnBu\", fmt=\".4f\", vmin=0, vmax=1)\n",
        "    plt.title('ROUGE-L Scores by Style and Complexity')\n",
        "    plt.ylabel('Code Complexity')\n",
        "    plt.xlabel('Documentation Style')\n",
        "    plt.savefig(f\"{output_dir}/charts/style_complexity_heatmap.png\", dpi=300, bbox_inches='tight')\n",
        "\n",
        "def create_html_report(results, avg_metrics, metrics_by_complexity, output_dir):\n",
        "    \"\"\"Create an HTML report with evaluation results and examples\"\"\"\n",
        "    # Select a few examples of different qualities for the report\n",
        "    sorted_results = sorted(results, key=lambda x: x[\"styles\"][\"standard\"][\"rougeL\"], reverse=True)\n",
        "\n",
        "    excellent_examples = sorted_results[:2]  # Top 2 examples\n",
        "    good_examples = sorted_results[len(sorted_results)//4:len(sorted_results)//4+2]  # Examples from 25% mark\n",
        "    poor_examples = sorted_results[-2:]  # Bottom 2 examples\n",
        "\n",
        "    # Create HTML content\n",
        "    html_content = f\"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head>\n",
        "        <title>TypeScript Documentation Model Evaluation Report</title>\n",
        "        <style>\n",
        "            body {{\n",
        "                font-family: Arial, sans-serif;\n",
        "                line-height: 1.6;\n",
        "                margin: 0;\n",
        "                padding: 20px;\n",
        "                color: #333;\n",
        "            }}\n",
        "            .container {{\n",
        "                max-width: 1200px;\n",
        "                margin: 0 auto;\n",
        "            }}\n",
        "            header {{\n",
        "                background-color: #f8f9fa;\n",
        "                padding: 20px;\n",
        "                margin-bottom: 30px;\n",
        "                border-radius: 5px;\n",
        "                box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n",
        "            }}\n",
        "            h1 {{\n",
        "                color: #2c3e50;\n",
        "                margin-top: 0;\n",
        "            }}\n",
        "            h2 {{\n",
        "                margin-top: 30px;\n",
        "                border-bottom: 2px solid #eee;\n",
        "                padding-bottom: 10px;\n",
        "                color: #3498db;\n",
        "            }}\n",
        "            h3 {{\n",
        "                color: #2980b9;\n",
        "            }}\n",
        "            .metrics-table {{\n",
        "                width: 100%;\n",
        "                border-collapse: collapse;\n",
        "                margin: 20px 0;\n",
        "            }}\n",
        "            .metrics-table th, .metrics-table td {{\n",
        "                border: 1px solid #ddd;\n",
        "                padding: 12px;\n",
        "                text-align: left;\n",
        "            }}\n",
        "            .metrics-table th {{\n",
        "                background-color: #f2f2f2;\n",
        "            }}\n",
        "            .metrics-table tr:nth-child(even) {{\n",
        "                background-color: #f9f9f9;\n",
        "            }}\n",
        "            .example {{\n",
        "                background-color: #f8f9fa;\n",
        "                padding: 15px;\n",
        "                margin: 20px 0;\n",
        "                border-radius: 5px;\n",
        "                border-left: 5px solid #3498db;\n",
        "            }}\n",
        "            .code-block {{\n",
        "                background-color: #f5f5f5;\n",
        "                padding: 15px;\n",
        "                border-radius: 5px;\n",
        "                overflow-x: auto;\n",
        "                font-family: monospace;\n",
        "                font-size: 14px;\n",
        "                line-height: 1.4;\n",
        "            }}\n",
        "            .chart {{\n",
        "                width: 100%;\n",
        "                max-width: 800px;\n",
        "                margin: 20px auto;\n",
        "                display: block;\n",
        "            }}\n",
        "            .excellent {{ border-left-color: #2ecc71; }}\n",
        "            .good {{ border-left-color: #f39c12; }}\n",
        "            .poor {{ border-left-color: #e74c3c; }}\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        <div class=\"container\">\n",
        "            <header>\n",
        "                <h1>TypeScript Documentation Model Evaluation Report</h1>\n",
        "                <p>Evaluation date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M')}</p>\n",
        "                <p>Number of examples evaluated: {len(results)}</p>\n",
        "            </header>\n",
        "\n",
        "            <h2>Overall Metrics</h2>\n",
        "            <table class=\"metrics-table\">\n",
        "                <tr>\n",
        "                    <th>Documentation Style</th>\n",
        "                    <th>ROUGE-1</th>\n",
        "                    <th>ROUGE-2</th>\n",
        "                    <th>ROUGE-L</th>\n",
        "                    <th>Avg. Length (words)</th>\n",
        "                </tr>\n",
        "    \"\"\"\n",
        "\n",
        "    # Add metrics rows\n",
        "    for style, metrics in avg_metrics.items():\n",
        "        html_content += f\"\"\"\n",
        "                <tr>\n",
        "                    <td>{style.capitalize()}</td>\n",
        "                    <td>{metrics['rouge1']:.4f}</td>\n",
        "                    <td>{metrics['rouge2']:.4f}</td>\n",
        "                    <td>{metrics['rougeL']:.4f}</td>\n",
        "                    <td>{metrics['length']:.1f}</td>\n",
        "                </tr>\n",
        "        \"\"\"\n",
        "\n",
        "    html_content += \"\"\"\n",
        "            </table>\n",
        "\n",
        "            <h2>Visualizations</h2>\n",
        "            <h3>Style Comparison</h3>\n",
        "            <img src=\"charts/style_comparison.png\" alt=\"Style Comparison Chart\" class=\"chart\">\n",
        "\n",
        "            <h3>Complexity Analysis</h3>\n",
        "            <img src=\"charts/complexity_comparison.png\" alt=\"Complexity Comparison Chart\" class=\"chart\">\n",
        "\n",
        "            <h3>Style vs. Complexity</h3>\n",
        "            <img src=\"charts/style_complexity_heatmap.png\" alt=\"Style vs. Complexity Heatmap\" class=\"chart\">\n",
        "\n",
        "            <h2>Example Outputs</h2>\n",
        "    \"\"\"\n",
        "\n",
        "    # Add excellent examples\n",
        "    html_content += \"\"\"\n",
        "            <h3>High-Quality Examples</h3>\n",
        "    \"\"\"\n",
        "\n",
        "    for example in excellent_examples:\n",
        "        html_content += f\"\"\"\n",
        "            <div class=\"example excellent\">\n",
        "                <h4>Code (Complexity: {example['complexity']})</h4>\n",
        "                <pre class=\"code-block\">{example['code']}</pre>\n",
        "\n",
        "                <h4>Reference Documentation</h4>\n",
        "                <pre class=\"code-block\">{example['reference']}</pre>\n",
        "\n",
        "                <h4>Generated Documentation (ROUGE-L: {example['styles']['standard']['rougeL']:.4f})</h4>\n",
        "                <pre class=\"code-block\">{example['styles']['standard']['generated']}</pre>\n",
        "            </div>\n",
        "        \"\"\"\n",
        "\n",
        "    # Add good examples\n",
        "    html_content += \"\"\"\n",
        "            <h3>Average-Quality Examples</h3>\n",
        "    \"\"\"\n",
        "\n",
        "    for example in good_examples:\n",
        "        html_content += f\"\"\"\n",
        "            <div class=\"example good\">\n",
        "                <h4>Code (Complexity: {example['complexity']})</h4>\n",
        "                <pre class=\"code-block\">{example['code']}</pre>\n",
        "\n",
        "                <h4>Reference Documentation</h4>\n",
        "                <pre class=\"code-block\">{example['reference']}</pre>\n",
        "\n",
        "                <h4>Generated Documentation (ROUGE-L: {example['styles']['standard']['rougeL']:.4f})</h4>\n",
        "                <pre class=\"code-block\">{example['styles']['standard']['generated']}</pre>\n",
        "            </div>\n",
        "        \"\"\"\n",
        "\n",
        "    # Add poor examples\n",
        "    html_content += \"\"\"\n",
        "            <h3>Low-Quality Examples</h3>\n",
        "    \"\"\"\n",
        "\n",
        "    for example in poor_examples:\n",
        "        html_content += f\"\"\"\n",
        "            <div class=\"example poor\">\n",
        "                <h4>Code (Complexity: {example['complexity']})</h4>\n",
        "                <pre class=\"code-block\">{example['code']}</pre>\n",
        "\n",
        "                <h4>Reference Documentation</h4>\n",
        "                <pre class=\"code-block\">{example['reference']}</pre>\n",
        "\n",
        "                <h4>Generated Documentation (ROUGE-L: {example['styles']['standard']['rougeL']:.4f})</h4>\n",
        "                <pre class=\"code-block\">{example['styles']['standard']['generated']}</pre>\n",
        "\n",
        "                <h4>Possible Improvements</h4>\n",
        "                <ul>\n",
        "                    <li>Better capture of parameter types and descriptions</li>\n",
        "                    <li>More accurate return type documentation</li>\n",
        "                    <li>Better understanding of the function's purpose</li>\n",
        "                </ul>\n",
        "            </div>\n",
        "        \"\"\"\n",
        "\n",
        "    # Add style comparison examples\n",
        "    html_content += \"\"\"\n",
        "            <h3>Documentation Style Comparison</h3>\n",
        "    \"\"\"\n",
        "\n",
        "    # Pick a medium-complexity example to show style differences\n",
        "    medium_examples = [r for r in results if r['complexity'] == 'medium']\n",
        "    if medium_examples:\n",
        "        style_example = medium_examples[0]\n",
        "\n",
        "        html_content += f\"\"\"\n",
        "            <div class=\"example\">\n",
        "                <h4>Original Code</h4>\n",
        "                <pre class=\"code-block\">{style_example['code']}</pre>\n",
        "\n",
        "                <h4>Reference Documentation</h4>\n",
        "                <pre class=\"code-block\">{style_example['reference']}</pre>\n",
        "        \"\"\"\n",
        "\n",
        "        for style, metrics in style_example['styles'].items():\n",
        "            html_content += f\"\"\"\n",
        "                <h4>{style.capitalize()} Style (ROUGE-L: {metrics['rougeL']:.4f})</h4>\n",
        "                <pre class=\"code-block\">{metrics['generated']}</pre>\n",
        "            \"\"\"\n",
        "\n",
        "        html_content += \"\"\"\n",
        "            </div>\n",
        "        \"\"\"\n",
        "\n",
        "    # Close HTML\n",
        "    html_content += \"\"\"\n",
        "            <h2>Conclusion</h2>\n",
        "            <p>The evaluation indicates that the model performs well across different types of TypeScript code and documentation styles.\n",
        "            The best performance is observed with the standard documentation style, while more complex formats like JSDoc and Markdown show slightly lower ROUGE scores but provide more structured and detailed documentation.</p>\n",
        "\n",
        "            <p>Performance tends to decrease as code complexity increases, which is expected. Future improvements could focus on better handling of complex TypeScript constructs and more accurate parameter inference.</p>\n",
        "        </div>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "\n",
        "    # Write HTML to file\n",
        "    with open(f\"{output_dir}/evaluation_report.html\", 'w') as f:\n",
        "        f.write(html_content)"
      ],
      "metadata": {
        "id": "vwe7ZbDy86MQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "model_path = '/content/drive/MyDrive/ts_documentation/models/enhanced_codet5/hparam_search_20250509_080333/best_model'  # Update to your best model path\n",
        "test_data_path = '/content/drive/MyDrive/ts_documentation/data/full_test_split.json'\n",
        "output_dir = '/content/drive/MyDrive/ts_documentation/evaluation_results'\n",
        "\n",
        "# Install required libraries if not already present\n",
        "!pip install -q matplotlib seaborn\n",
        "\n",
        "# Run the comprehensive evaluation\n",
        "avg_metrics = run_comprehensive_evaluation(model_path, test_data_path, output_dir)\n",
        "\n",
        "# Print summary of results\n",
        "print(\"\\nEvaluation Summary:\")\n",
        "for style, metrics in avg_metrics.items():\n",
        "    print(f\"- {style.capitalize()} Style:\")\n",
        "    print(f\"  - ROUGE-L: {metrics['rougeL']:.4f}\")\n",
        "    print(f\"  - Avg Length: {metrics['length']:.1f} words\")"
      ],
      "metadata": {
        "id": "qX0rp_hM9N9N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}